{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rolandooviedo/Documentos/Github/p3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "data_path = os.getcwd()[:-len('Notebooks')] + 'Data/fake-and-real-news-dataset/'\n",
    "os.listdir(data_path)\n",
    "\n",
    "fake = pd.read_csv(data_path + 'Fake.csv')\n",
    "real = pd.read_csv(data_path + 'True.csv')\n",
    "\n",
    "fake['target'] = '0'\n",
    "real['target'] = '1'\n",
    "\n",
    "news = fake.append(real)\n",
    "news = news.sort_values(by = 'date')\n",
    "\n",
    "news.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20/12/2017'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.strptime('December 20 2017', '%B %d %Y').strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...</td>\n",
       "      <td>We ve been covering the curious case of DOJ of...</td>\n",
       "      <td>politics</td>\n",
       "      <td>14-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The DIRTY TRUTH About DACA Recipients…Where Th...</td>\n",
       "      <td>Yesterday, a second U.S. judge on Tuesday bloc...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUH? RAPPER JAY-Z Compares Trayvon Martin to M...</td>\n",
       "      <td>Rapper Jay-Z attended the Peace Walk &amp; Peace T...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC STATE REP Who is a Former Cop Proposes Smar...</td>\n",
       "      <td>WHO BETTER THAN A FORMER POLICE OFFICER TO UND...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUST IN: Senate Rejects Immigration Bill…Trump...</td>\n",
       "      <td>How could the Senate be so lame in their effor...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...   \n",
       "1  The DIRTY TRUTH About DACA Recipients…Where Th...   \n",
       "2  HUH? RAPPER JAY-Z Compares Trayvon Martin to M...   \n",
       "3  SC STATE REP Who is a Former Cop Proposes Smar...   \n",
       "4  JUST IN: Senate Rejects Immigration Bill…Trump...   \n",
       "\n",
       "                                                text   subject       date  \\\n",
       "0  We ve been covering the curious case of DOJ of...  politics  14-Feb-18   \n",
       "1  Yesterday, a second U.S. judge on Tuesday bloc...  politics  15-Feb-18   \n",
       "2  Rapper Jay-Z attended the Peace Walk & Peace T...  politics  15-Feb-18   \n",
       "3  WHO BETTER THAN A FORMER POLICE OFFICER TO UND...  politics  15-Feb-18   \n",
       "4  How could the Senate be so lame in their effor...  politics  15-Feb-18   \n",
       "\n",
       "  target clean_date  \n",
       "0      0 2018-02-14  \n",
       "1      0 2018-02-15  \n",
       "2      0 2018-02-15  \n",
       "3      0 2018-02-15  \n",
       "4      0 2018-02-15  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "dates = []\n",
    "others = []\n",
    "for e in set(news['date']):\n",
    "    if ',' in e:\n",
    "        if len(e.split(' ')[0]) == 3:\n",
    "            r = str(e.replace(',', '').strip())\n",
    "            dates.append(dt.datetime.strptime(r, '%b %d %Y').strftime('%d/%m/%Y'))\n",
    "        else:\n",
    "            r = str(e.replace(',', '').strip())\n",
    "            dates.append(dt.datetime.strptime(r, '%B %d %Y').strftime('%d/%m/%Y'))\n",
    "    \n",
    "    elif 'Feb' in e:\n",
    "        dates.append(dt.datetime.strptime(e, '%d-%b-%y').strftime('%d/%m/%Y'))\n",
    "    \n",
    "    else:\n",
    "        others.append(e)\n",
    "\n",
    "aux = news[~news['date'].isin(others)].copy()\n",
    "\n",
    "new_dates = []\n",
    "others = []\n",
    "for e in aux['date']:\n",
    "    if ',' in e:\n",
    "        if len(e.split(' ')[0]) == 3:\n",
    "            r = str(e.replace(',', '').strip())\n",
    "            new_dates.append(dt.datetime.strptime(r, '%b %d %Y').strftime('%d/%m/%Y'))\n",
    "        else:\n",
    "            r = str(e.replace(',', '').strip())\n",
    "            new_dates.append(dt.datetime.strptime(r, '%B %d %Y').strftime('%d/%m/%Y'))\n",
    "    \n",
    "    elif 'Feb' in e:\n",
    "        new_dates.append(dt.datetime.strptime(e, '%d-%b-%y').strftime('%d/%m/%Y'))\n",
    "    \n",
    "    else:\n",
    "        others.append(e)\n",
    "\n",
    "\n",
    "aux['clean_date'] = pd.to_datetime(pd.Series(new_dates))\n",
    "\n",
    "aux.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del texto y steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rolandooviedo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rolandooviedo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rolandooviedo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text_clean = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text_clean = re.split('\\W+', text.lower())\n",
    "    text_clean = [word for word in text_clean if word not in stopwords]\n",
    "    text_clean = \" \".join([lemmatizer.lemmatize(i, 'v') for i in text_clean])\n",
    "    return text_clean\n",
    "\n",
    "news['clean_text'] = news['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yesterday second u judge tuesday block president donald trump decision end program protect immigrants bring unite state illegally children deportation u district judge nicholas garaufis brooklyn rule defer action childhood arrivals program daca cannot end march republican administration plan victory democratic state attorneys general immigrants sue federal government decision similar jan 9 rule u district judge william alsup san francisco daca must remain place litigation challenge trump decision continue legal battle daca complicate debate currently underway congress whether change nation immigration laws supreme court friday due consider whether take administration appeal san francisco rule court could announce soon friday afternoon whether hear case reuterssince judge unilaterally decide program base executive order radical lawless president expiration date extend peel back layer lie tell american public democrats like congresswoman nancy pelosi senator chuck schumer ally media discover shock truth dreamers voters fight hard keep america accord hill narrative surround defer action childhood arrivals daca program hold put place protect kid bring fault daca supporters imply applicants mostly hispanic citizens distress republics short distance away wealthy unite state violation immigration laws somehow understandable program applicants also portray brilliant valedictorians proud members military outset narrative ring hollow column washington post mickey kaus describe public relations style hooey new data release u citizenship immigration service uscis definitively establish daca narrative false particularly overblown claim deport daca recipients would inevitably strangers strange land uscis list 149 countries origin daca applicants english national language least 26 countries include unite kingdom canada australia new zealand ireland large number applicants india hong kong philippines enormous english speak communities statistics also undermine claim unite state must take care dacas condemn life isolation poverty fact many daca applicant birth countries list higher end common standard live index mean daca advocate argue unite state moral obligation undermine laws order avoid return illegal alien countries whose citizens consider higher standard live many americans one wonder never occur congress primary obligation protect native bear american kid illegal alien compete entry level job seat colleges universities curiously elect representatives seem remarkably unconcerned protect america standard live disturb data release uscis however number daca applicants come countries associate terrorism overt anti americanism cause concern give lean lite vet use quickly approve daca applications base facts rather myth seem clear individuals apply receive daca oppress well mean high achievers media open border lobby portray '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['clean_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crook',\n",
       " 'doj',\n",
       " 'official',\n",
       " 'reveal',\n",
       " 'wife',\n",
       " 'pay',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'get',\n",
       " 'dirt',\n",
       " 'trump',\n",
       " 'video',\n",
       " 'cover',\n",
       " 'curious',\n",
       " 'case',\n",
       " 'doj',\n",
       " 'official',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'wife',\n",
       " 'nellie',\n",
       " 'ohr',\n",
       " 'corrupt',\n",
       " 'underhanded',\n",
       " 'nellie',\n",
       " 'work',\n",
       " 'opposition',\n",
       " 'research',\n",
       " 'firm',\n",
       " 'hire',\n",
       " 'democrats',\n",
       " 'get',\n",
       " 'dirt',\n",
       " 'trump',\n",
       " 'see',\n",
       " 'husband',\n",
       " 'would',\n",
       " 'take',\n",
       " 'whatever',\n",
       " 'nellie',\n",
       " 'produce',\n",
       " 'trump',\n",
       " 'hand',\n",
       " 'fbi',\n",
       " 'unreal',\n",
       " 'jail',\n",
       " 'time',\n",
       " 'omission',\n",
       " 'conflict',\n",
       " 'interest',\n",
       " 'ethics',\n",
       " 'violation',\n",
       " 'far',\n",
       " 'appear',\n",
       " 'though',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'demote',\n",
       " 'find',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'hide',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'payments',\n",
       " 'wife',\n",
       " 'daily',\n",
       " 'caller',\n",
       " 'report',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'department',\n",
       " 'justice',\n",
       " 'official',\n",
       " 'bring',\n",
       " 'opposition',\n",
       " 'research',\n",
       " 'president',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'fbi',\n",
       " 'disclose',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'perform',\n",
       " 'research',\n",
       " 'democratic',\n",
       " 'national',\n",
       " 'committee',\n",
       " 'behest',\n",
       " 'pay',\n",
       " 'wife',\n",
       " 'obtain',\n",
       " 'conflict',\n",
       " 'interest',\n",
       " 'waiver',\n",
       " 'superiors',\n",
       " 'justice',\n",
       " 'department',\n",
       " 'document',\n",
       " 'obtain',\n",
       " 'daily',\n",
       " 'caller',\n",
       " 'news',\n",
       " 'foundation',\n",
       " 'show',\n",
       " 'omission',\n",
       " 'may',\n",
       " 'explain',\n",
       " 'ohr',\n",
       " 'demote',\n",
       " 'post',\n",
       " 'associate',\n",
       " 'deputy',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'relationship',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'wife',\n",
       " 'emerge',\n",
       " 'fusion',\n",
       " 'founder',\n",
       " 'glenn',\n",
       " 'simpson',\n",
       " 'acknowledge',\n",
       " 'meet',\n",
       " 'ohr',\n",
       " 'willfully',\n",
       " 'falsify',\n",
       " 'government',\n",
       " 'ethics',\n",
       " 'form',\n",
       " 'carry',\n",
       " 'penalty',\n",
       " 'jail',\n",
       " 'time',\n",
       " 'convict',\n",
       " 'democratic',\n",
       " 'national',\n",
       " 'committee',\n",
       " 'dnc',\n",
       " 'hire',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'gather',\n",
       " 'disseminate',\n",
       " 'damn',\n",
       " 'info',\n",
       " 'trump',\n",
       " 'turn',\n",
       " 'pay',\n",
       " 'nellie',\n",
       " 'ohr',\n",
       " 'former',\n",
       " 'cia',\n",
       " 'employee',\n",
       " 'expertise',\n",
       " 'russia',\n",
       " 'unknown',\n",
       " 'role',\n",
       " 'relate',\n",
       " 'dossier',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'bring',\n",
       " 'information',\n",
       " 'fbi',\n",
       " 'kick',\n",
       " 'probe',\n",
       " 'media',\n",
       " 'firestorm',\n",
       " 'doj',\n",
       " 'use',\n",
       " 'obtain',\n",
       " 'warrant',\n",
       " 'wiretap',\n",
       " 'trump',\n",
       " 'adviser',\n",
       " 'disclose',\n",
       " 'judge',\n",
       " 'dnc',\n",
       " 'former',\n",
       " 'secretary',\n",
       " 'state',\n",
       " 'hillary',\n",
       " 'clinton',\n",
       " 'campaign',\n",
       " 'fund',\n",
       " 'research',\n",
       " 'ohr',\n",
       " 'financial',\n",
       " 'relationship',\n",
       " 'firm',\n",
       " 'perform',\n",
       " 'could',\n",
       " 'turn',\n",
       " 'ohr',\n",
       " 'appear',\n",
       " 'tell',\n",
       " 'supervisors',\n",
       " 'suggest',\n",
       " 'financial',\n",
       " 'payments',\n",
       " 'motivate',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'actively',\n",
       " 'push',\n",
       " 'case',\n",
       " 'previous',\n",
       " 'report',\n",
       " 'nellie',\n",
       " 'ohr',\n",
       " 'big',\n",
       " 'deal',\n",
       " 'even',\n",
       " 'though',\n",
       " 'sure',\n",
       " 'main',\n",
       " 'stream',\n",
       " 'media',\n",
       " 'ignore',\n",
       " 'connection',\n",
       " 'demote',\n",
       " 'doj',\n",
       " 'official',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'wife',\n",
       " 'work',\n",
       " 'opposition',\n",
       " 'research',\n",
       " 'firm',\n",
       " 'responsible',\n",
       " 'anti',\n",
       " 'trump',\n",
       " 'dossier',\n",
       " 'plot',\n",
       " 'thicken',\n",
       " 'span',\n",
       " 'data',\n",
       " 'mce',\n",
       " 'type',\n",
       " 'bookmark',\n",
       " 'style',\n",
       " 'display',\n",
       " 'inline',\n",
       " 'block',\n",
       " 'width',\n",
       " '0px',\n",
       " 'overflow',\n",
       " 'hide',\n",
       " 'line',\n",
       " 'height',\n",
       " '0',\n",
       " 'class',\n",
       " 'mce_selres_start',\n",
       " 'span',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'report',\n",
       " 'senior',\n",
       " 'justice',\n",
       " 'department',\n",
       " 'official',\n",
       " 'demote',\n",
       " 'last',\n",
       " 'week',\n",
       " 'conceal',\n",
       " 'meet',\n",
       " 'men',\n",
       " 'behind',\n",
       " 'anti',\n",
       " 'trump',\n",
       " 'dossier',\n",
       " 'even',\n",
       " 'closer',\n",
       " 'tie',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'firm',\n",
       " 'responsible',\n",
       " 'incendiary',\n",
       " 'document',\n",
       " 'disclose',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'confirm',\n",
       " 'official',\n",
       " 'wife',\n",
       " 'work',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " '2016',\n",
       " 'election',\n",
       " 'contact',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'investigators',\n",
       " 'house',\n",
       " 'permanent',\n",
       " 'select',\n",
       " 'committee',\n",
       " 'intelligence',\n",
       " 'hpsci',\n",
       " 'confirm',\n",
       " 'nellie',\n",
       " 'h',\n",
       " 'ohr',\n",
       " 'wife',\n",
       " 'demote',\n",
       " 'official',\n",
       " 'bruce',\n",
       " 'g',\n",
       " 'ohr',\n",
       " 'work',\n",
       " 'opposition',\n",
       " 'research',\n",
       " 'firm',\n",
       " 'last',\n",
       " 'year',\n",
       " 'precise',\n",
       " 'nature',\n",
       " 'mrs',\n",
       " 'ohr',\n",
       " 'duties',\n",
       " 'include',\n",
       " 'whether',\n",
       " 'work',\n",
       " 'dossier',\n",
       " 'remain',\n",
       " 'unclear',\n",
       " 'review',\n",
       " 'publish',\n",
       " 'work',\n",
       " 'available',\n",
       " 'online',\n",
       " 'reveal',\n",
       " 'mrs',\n",
       " 'ohr',\n",
       " 'write',\n",
       " 'extensively',\n",
       " 'russia',\n",
       " 'relate',\n",
       " 'subject',\n",
       " 'hpsci',\n",
       " 'staff',\n",
       " 'confirm',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'pay',\n",
       " 'fusion',\n",
       " 'gps',\n",
       " 'summer',\n",
       " 'fall',\n",
       " '2016',\n",
       " 'read',\n",
       " 'fox',\n",
       " 'newsjames',\n",
       " 'rosen',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'kabooooom',\n",
       " 'mueller',\n",
       " 'investigation',\n",
       " 'crumble',\n",
       " 'first',\n",
       " 'strzok',\n",
       " 'paige',\n",
       " 'weissmann',\n",
       " 'rhee',\n",
       " 'bruce',\n",
       " 'ohr',\n",
       " 'bust',\n",
       " 'w',\n",
       " 'hand',\n",
       " 'fake',\n",
       " 'trump',\n",
       " 'dossier',\n",
       " 'cookie',\n",
       " 'jar',\n",
       " 'firemueller',\n",
       " 'shutitdown',\n",
       " 'investigate',\n",
       " 'investigators',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'u9ygfa6421',\n",
       " 'obamagate',\n",
       " 'stockmonstervip',\n",
       " 'december',\n",
       " '7',\n",
       " '2017read',\n",
       " 'daily',\n",
       " 'caller']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['Final_Text'] = news['title'] + news['text']\n",
    "news['clean_title'] = news['title'].map(clean_text)\n",
    "news['clean_Final_Text'] = news['Final_Text'].map(clean_text)\n",
    "news['tokenized text'] = news['clean_Final_Text'].map(word_tokenize)\n",
    "news['tokenized text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>Final_Text</th>\n",
       "      <th>clean_Final_Text</th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>tokenized_solo_text</th>\n",
       "      <th>tokenized_solo_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...</td>\n",
       "      <td>We ve been covering the curious case of DOJ of...</td>\n",
       "      <td>politics</td>\n",
       "      <td>14-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>cover curious case doj official bruce ohr wife...</td>\n",
       "      <td>crook doj official reveal wife pay fusion gps ...</td>\n",
       "      <td>JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...</td>\n",
       "      <td>crook doj official reveal wife pay fusion gps ...</td>\n",
       "      <td>[crook, doj, official, reveal, wife, pay, fusi...</td>\n",
       "      <td>[cover, curious, case, doj, official, bruce, o...</td>\n",
       "      <td>[crook, doj, official, reveal, wife, pay, fusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The DIRTY TRUTH About DACA Recipients…Where Th...</td>\n",
       "      <td>Yesterday, a second U.S. judge on Tuesday bloc...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>yesterday second u judge tuesday block preside...</td>\n",
       "      <td>dirty truth daca recipients really come crimin...</td>\n",
       "      <td>The DIRTY TRUTH About DACA Recipients…Where Th...</td>\n",
       "      <td>dirty truth daca recipients really come crimin...</td>\n",
       "      <td>[dirty, truth, daca, recipients, really, come,...</td>\n",
       "      <td>[yesterday, second, u, judge, tuesday, block, ...</td>\n",
       "      <td>[dirty, truth, daca, recipients, really, come,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUH? RAPPER JAY-Z Compares Trayvon Martin to M...</td>\n",
       "      <td>Rapper Jay-Z attended the Peace Walk &amp; Peace T...</td>\n",
       "      <td>politics</td>\n",
       "      <td>15-Feb-18</td>\n",
       "      <td>0</td>\n",
       "      <td>rapper jay z attend peace walk peace talk miam...</td>\n",
       "      <td>huh rapper jay z compare trayvon martin mlk ga...</td>\n",
       "      <td>HUH? RAPPER JAY-Z Compares Trayvon Martin to M...</td>\n",
       "      <td>huh rapper jay z compare trayvon martin mlk ga...</td>\n",
       "      <td>[huh, rapper, jay, z, compare, trayvon, martin...</td>\n",
       "      <td>[rapper, jay, z, attend, peace, walk, peace, t...</td>\n",
       "      <td>[huh, rapper, jay, z, compare, trayvon, martin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...   \n",
       "1  The DIRTY TRUTH About DACA Recipients…Where Th...   \n",
       "2  HUH? RAPPER JAY-Z Compares Trayvon Martin to M...   \n",
       "\n",
       "                                                text   subject       date  \\\n",
       "0  We ve been covering the curious case of DOJ of...  politics  14-Feb-18   \n",
       "1  Yesterday, a second U.S. judge on Tuesday bloc...  politics  15-Feb-18   \n",
       "2  Rapper Jay-Z attended the Peace Walk & Peace T...  politics  15-Feb-18   \n",
       "\n",
       "  target                                         clean_text  \\\n",
       "0      0  cover curious case doj official bruce ohr wife...   \n",
       "1      0  yesterday second u judge tuesday block preside...   \n",
       "2      0  rapper jay z attend peace walk peace talk miam...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  crook doj official reveal wife pay fusion gps ...   \n",
       "1  dirty truth daca recipients really come crimin...   \n",
       "2  huh rapper jay z compare trayvon martin mlk ga...   \n",
       "\n",
       "                                          Final_Text  \\\n",
       "0  JUST IN: CROOKED DOJ OFFICIAL Didn’t Reveal Th...   \n",
       "1  The DIRTY TRUTH About DACA Recipients…Where Th...   \n",
       "2  HUH? RAPPER JAY-Z Compares Trayvon Martin to M...   \n",
       "\n",
       "                                    clean_Final_Text  \\\n",
       "0  crook doj official reveal wife pay fusion gps ...   \n",
       "1  dirty truth daca recipients really come crimin...   \n",
       "2  huh rapper jay z compare trayvon martin mlk ga...   \n",
       "\n",
       "                                      tokenized text  \\\n",
       "0  [crook, doj, official, reveal, wife, pay, fusi...   \n",
       "1  [dirty, truth, daca, recipients, really, come,...   \n",
       "2  [huh, rapper, jay, z, compare, trayvon, martin...   \n",
       "\n",
       "                                 tokenized_solo_text  \\\n",
       "0  [cover, curious, case, doj, official, bruce, o...   \n",
       "1  [yesterday, second, u, judge, tuesday, block, ...   \n",
       "2  [rapper, jay, z, attend, peace, walk, peace, t...   \n",
       "\n",
       "                                tokenized_solo_title  \n",
       "0  [crook, doj, official, reveal, wife, pay, fusi...  \n",
       "1  [dirty, truth, daca, recipients, really, come,...  \n",
       "2  [huh, rapper, jay, z, compare, trayvon, martin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['tokenized_solo_text'] = news['clean_text'].map(word_tokenize)\n",
    "news['tokenized_solo_title'] = news['clean_title'].map(word_tokenize)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-?!1234567890.,;:\"$\\'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "letters = '-?!1234567890.,;:\"$' + \"'\" + string.ascii_letters[:26]\n",
    "n_letters = len(letters)\n",
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### La columna que queremos es clean_Final_Text\n",
    "\n",
    "textc = 'clean_Final_Text'\n",
    "tar = 'target'\n",
    "tokens = 'tokenized text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = {}\n",
    "tokens_dict = {}\n",
    "values = [0, 1]\n",
    "\n",
    "for v in values:\n",
    "    text = list(news.loc[news[tar] == v][textc]) #texto\n",
    "    tok = list(news.loc[news[tar] == v][tokens]) #tokens\n",
    "    fake[v] = text\n",
    "    tokens_dict[v] = tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería en los valores\n",
    "\n",
    "En este espacio lo que haremos será crear las matcies vectoriales del texto de entrada y salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el texto en una matriz con las siguientes entradas:\n",
    "# longitud del texto, 1, número de letras, (la típica codificación one-hot)\n",
    "\n",
    "def texttotensor(t):\n",
    "    tensor = torch.zeros(len(t), 1, n_letters)\n",
    "    for la, l in enumerate(t):\n",
    "      l = letters.find(l)\n",
    "      tensor[la][0][l] = 1.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del texto y de los tokensm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[textc] = news[textc].apply(lambda x:re.sub('[^a-z0-9:''\"\";,.$#@-]+', '', x))\n",
    "news[tokens] = news[tokens].apply(lambda x:[re.sub('[^a-z0-9:''\"\";,.$#@-]+', '', j) for j in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = 'eyj1cgxvywrfawqioijlehbyzxnzlte0mzaxnjg0odc4otqilcj2awrlb190exblijoimsisinnyy19pbwfnzv91cmwioijodhrwoi8vbwvkaweylmfiyzjuzxdzlmnvbs9wag90by8ymde1lza0lzi3lze2edkvtw9yzv92aw9szw5jzv9pbl90agvfc3ryzwv0c19vzl9cywx0aw1vcl8yodc0ntgwmdaxxze3nde3nzc0x3zlcjeumf82ndbfndgwlmpwzyisimrlzl90axrszsi6ik1vcmugdmlvbgvuy2ugaw4gdghlihn0cmvldhmgb2ygqmfsdgltb3jligj5ie1vbmrhd21pbibnywxsiiwidgl0bguioijnb3jlihzpb2xlbmnligluihrozsbzdhjlzxrzig9miejhbhrpbw9yzsbiesbnb25kyxdtaw4gtwfsbcisimrlzl9kzwnzcmlwdglvbii6iiisinb1ymxpc2hlzf91cmxzijpbeyjmb3jtyxqioijtcdq7iiwizw1izwrfdxjsijoiahr0cdovl21lzglhmi5hymmybmv3cy5jb20vdmlkzw8vdmlkzw9fc3r1zglvlziwmtuvmdqvmjcvtw9yzv92aw9szw5jzv9pbl90agvfc3ryzwv0c19vzl9cywx0aw1vcl8yodc0ntgubxa0iiwichjvdg9jb2wioijodhrwoiisimnkbl9uyw1lijoiqwthbwfpiiwia2jwcyi6ntawfv0simnhchrpb25zijpbeyjsyw5ndwfnzsi6imvuiiwizm9ybwf0ijoirezyucisinvybci6imh0dha6ly9tzwrpytiuywjjmm5ld3muy29tl3zpzgvvl3zpzgvvx3n0dwrpby8ymde1lza0lzi3l2nhchrpb25zl01vcmvfdmlvbgvuy2vfaw5fdghlx3n0cmvldhnfb2zfqmfsdgltb3jfmjg3ndu4mdawmc5kznhwin1dlcj0c19wdwjsaxnozwqioiiymde1xza0xzi3in0'\n",
    "for i,j in enumerate(news[tokens]):\n",
    "    if ejemplo in j: news[tokens][i].remove(ejemplo)\n",
    "\n",
    "max_len = 1000\n",
    "for i in range(len(news)):\n",
    "    r = news[tokens][i]\n",
    "    for e in r:\n",
    "        if len(e) > max_len: \n",
    "            print(e)\n",
    "            max_len = len(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2324, 1, 46])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texttotensor(news[textc][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.DataFrame()\n",
    "inputs[tar] = news[tar]\n",
    "\n",
    "inputs['news'] = news[textc].map(texttotensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenstotensor(token):\n",
    "    #considerando la entrada de [palabra1, palabra2, ...., palabraN]\n",
    "    s = ' '.join(token)\n",
    "    tensor = texttotensor(s)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['tokens'] = news[tokens].map(tokenstotensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
